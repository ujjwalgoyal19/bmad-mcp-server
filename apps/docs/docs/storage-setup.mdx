---
title: "Storage Configuration Guide"
description: "Configure BMAD MCP's storage backends for optimal performance and scalability."
icon: "Database"
---

# Storage Configuration Guide

<Info>
  **Flexible Storage Architecture**: BMAD MCP supports multiple storage backends
  with automatic failover and performance optimization.
</Info>

## Storage Architecture

BMAD MCP uses a multi-tier storage strategy:

```mermaid
graph TB
    A[Application] --> B[Storage Port]
    B --> C{Storage Type}
    C --> D[S3/MinIO]
    C --> E[Local Filesystem]
    C --> F[Database BLOBs]

    D --> G[Cloud Storage]
    E --> H[Local Development]
    F --> I[Embedded Storage]
```

## Quick Setup

### Local Development

```bash
# Start MinIO for local S3-compatible storage
bun run compose:up

# Environment variables are pre-configured
# S3_ENDPOINT=http://localhost:9000
# S3_BUCKET=bmad
# S3_ACCESS_KEY=minio
# S3_SECRET_KEY=minio123
```

### Production Setup

```bash
# AWS S3
export S3_ENDPOINT=https://s3.amazonaws.com
export S3_BUCKET=your-bucket-name
export S3_ACCESS_KEY=your-access-key
export S3_SECRET_KEY=your-secret-key
export S3_REGION=us-east-1

# Or DigitalOcean Spaces
export S3_ENDPOINT=https://your-region.digitaloceanspaces.com
export S3_BUCKET=your-space-name
```

## Storage Types

### S3-Compatible Storage (Recommended)

<CardGroup cols={2}>
  <Card title="AWS S3" icon="Cloud">
    Global cloud storage with high durability
  </Card>
  <Card title="MinIO" icon="Server">
    Self-hosted S3-compatible object storage
  </Card>
</CardGroup>

#### Configuration

```typescript
// S3 Storage Configuration
const s3Config = {
  endpoint: process.env.S3_ENDPOINT,
  bucket: process.env.S3_BUCKET,
  accessKey: process.env.S3_ACCESS_KEY,
  secretKey: process.env.S3_SECRET_KEY,
  region: process.env.S3_REGION || "us-east-1",
  // Performance optimizations
  maxRetries: 3,
  timeout: 30000,
  partSize: 5 * 1024 * 1024, // 5MB parts
  queueSize: 4,
};
```

#### Features

<Check>**Automatic Retry**: Failed uploads automatically retry</Check>

<Check>**Multipart Upload**: Large files uploaded in parts</Check>

<Check>**CDN Integration**: Global content delivery</Check>

<Check>**Versioning**: File version history</Check>

### Local Filesystem Storage

```typescript
// Local storage for development
const localConfig = {
  basePath: "./storage",
  maxFileSize: 100 * 1024 * 1024, // 100MB
  allowedTypes: [".md", ".txt", ".json", ".pdf"],
  createDirectories: true,
};
```

### Database BLOB Storage

```typescript
// Embedded storage for small files
const blobConfig = {
  maxSize: 10 * 1024 * 1024, // 10MB
  compression: true,
  encryption: false,
};
```

## Performance Optimization

### Caching Strategy

```typescript
const cacheConfig = {
  redis: {
    host: process.env.REDIS_HOST,
    port: parseInt(process.env.REDIS_PORT || "6379"),
    ttl: 3600, // 1 hour
    maxMemory: "512mb",
  },
  local: {
    maxSize: 100 * 1024 * 1024, // 100MB
    ttl: 1800, // 30 minutes
  },
};
```

### Connection Pooling

```typescript
// Database connection pool
const dbPool = {
  min: 2,
  max: 10,
  idleTimeoutMillis: 30000,
  acquireTimeoutMillis: 60000,
};

// Storage connection pool
const storagePool = {
  maxConnections: 20,
  idleTimeout: 30000,
  acquireTimeout: 60000,
};
```

## Backup & Recovery

### Automated Backups

```bash
# Daily backup script
#!/bin/bash
DATE=$(date +%Y%m%d)
BACKUP_DIR="/backups/$DATE"

# Database backup
pg_dump bmad > "$BACKUP_DIR/database.sql"

# Storage backup
mc mirror bmad "$BACKUP_DIR/storage/"

# Compress and upload to S3
tar -czf "$BACKUP_DIR.tar.gz" "$BACKUP_DIR"
aws s3 cp "$BACKUP_DIR.tar.gz" "s3://backups/"
```

### Recovery Procedures

```bash
# Database recovery
psql bmad < backup/database.sql

# Storage recovery
mc mirror backup/storage bmad

# Verify integrity
./verify-backup.sh
```

## Monitoring & Analytics

### Storage Metrics

```typescript
const metrics = {
  uploadCount: 0,
  downloadCount: 0,
  totalBytesUploaded: 0,
  totalBytesDownloaded: 0,
  averageUploadTime: 0,
  averageDownloadTime: 0,
  errorRate: 0,
};
```

### Health Checks

```bash
# Storage health check
curl http://localhost:3000/health/storage

# Detailed metrics
curl http://localhost:3000/metrics/storage
```

## Security Configuration

### Access Control

```typescript
const securityConfig = {
  encryption: {
    enabled: true,
    algorithm: "AES-256-GCM",
    keyRotation: "30d",
  },
  accessControl: {
    allowPublicRead: false,
    signedUrls: true,
    urlExpiration: "1h",
  },
  audit: {
    enabled: true,
    logRetention: "90d",
  },
};
```

### Network Security

```bash
# Firewall configuration
sudo ufw allow 9000/tcp  # MinIO
sudo ufw allow 5432/tcp  # PostgreSQL
sudo ufw allow 6379/tcp  # Redis

# SSL/TLS configuration
export S3_USE_SSL=true
export S3_VERIFY_SSL=true
```

## Scaling Strategies

### Horizontal Scaling

```yaml
# Docker Compose for scaling
version: "3.8"
services:
  minio1:
    image: minio/minio
    volumes:
      - ./data1:/data
    command: server /data --console-address ":9001"

  minio2:
    image: minio/minio
    volumes:
      - ./data2:/data
    command: server /data --console-address ":9002"

  nginx:
    image: nginx
    ports:
      - "9000:9000"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
```

### Load Balancing

```nginx
# nginx.conf for load balancing
upstream minio_backend {
    server minio1:9000;
    server minio2:9000;
}

server {
    listen 9000;
    location / {
        proxy_pass http://minio_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

## Troubleshooting

### Common Issues

**Connection failed?**

```bash
# Check MinIO status
docker ps | grep minio

# Test connection
curl http://localhost:9000/minio/health/ready

# Check logs
docker logs minio
```

**Upload failed?**

```bash
# Check disk space
df -h

# Check permissions
ls -la /storage/

# Test with small file
echo "test" | aws s3 cp - s3://bmad/test.txt
```

**Slow performance?**

```bash
# Check network
ping s3.amazonaws.com

# Monitor CPU/memory
top

# Check connection pool
curl http://localhost:3000/metrics/connections
```

### Debug Mode

```bash
# Enable storage debugging
export DEBUG=bmcp:storage

# View detailed logs
tail -f logs/storage.log

# Performance profiling
curl http://localhost:3000/debug/storage/profile
```

## Migration Guide

### From Local to Cloud

```bash
# Export local data
./export-local-data.sh > local-data.json

# Import to cloud storage
./import-cloud-data.sh local-data.json

# Update configuration
export S3_ENDPOINT=https://s3.amazonaws.com
export S3_BUCKET=your-cloud-bucket

# Test migration
./verify-migration.sh
```

### Database Migration

```bash
# Export current database
pg_dump bmad > current.sql

# Create new database
createdb bmad_new

# Import data
psql bmad_new < current.sql

# Switch configuration
export DATABASE_URL="postgresql://user:pass@host:5432/bmad_new"

# Test and switch
./test-migration.sh && ./switch-database.sh
```

## Best Practices

### File Organization

```bash
# Recommended structure
storage/
â”œâ”€â”€ projects/
â”‚   â”œâ”€â”€ {project-id}/
â”‚   â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”œâ”€â”€ sessions/
â”‚   â”‚   â””â”€â”€ snapshots/
â”œâ”€â”€ users/
â”‚   â”œâ”€â”€ {user-id}/
â”‚   â”‚   â”œâ”€â”€ avatars/
â”‚   â”‚   â””â”€â”€ exports/
â””â”€â”€ system/
    â”œâ”€â”€ backups/
    â”œâ”€â”€ logs/
    â””â”€â”€ cache/
```

### Naming Conventions

```typescript
const namingStrategy = {
  projectFiles: "project-{projectId}-{timestamp}-{filename}",
  userFiles: "user-{userId}-{type}-{filename}",
  tempFiles: "temp-{sessionId}-{random}",
  backupFiles: "backup-{date}-{type}.tar.gz",
};
```

### Cleanup Policies

```typescript
const cleanupConfig = {
  tempFiles: {
    maxAge: "1h",
    cleanupInterval: "30m",
  },
  cacheFiles: {
    maxAge: "24h",
    maxSize: "1GB",
  },
  logFiles: {
    maxAge: "30d",
    compression: true,
  },
};
```

## Next Steps

<CardGroup cols={2}>
  <Card title="ðŸš€ Quick Start" icon="Rocket" href="/quick-start">
    Get started with storage
  </Card>
  <Card title="ðŸ“Š API Reference" icon="Code" href="/api/core">
    Storage API documentation
  </Card>
</CardGroup>

**Configure BMAD MCP's storage for optimal performance and reliability!**
